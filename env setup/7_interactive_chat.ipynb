{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 대화형 RAG 챗봇 구현\n",
                "\n",
                "이 노트북은 지금까지 구현한 모든 구성 요소를 통합하여 대화형 RAG 챗봇을 구현합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import os\n",
                "import yaml\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "from libs.common_utils import OpenSearch_Manager\n",
                "from IPython.display import display, clear_output\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 환경 설정"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 모델 설정 로드\n",
                "def load_model_config():\n",
                "    with open(\"libs/config.yml\", \"r\") as file:\n",
                "        return yaml.safe_load(file)\n",
                "\n",
                "model_config = load_model_config()\n",
                "\n",
                "# 기본 설정\n",
                "DEFAULT_TOP_K = 5\n",
                "DEFAULT_TEMPERATURE = 0.7\n",
                "DEFAULT_TOP_P = 0.95\n",
                "\n",
                "# OpenSearch 매니저 초기화\n",
                "os_manager = OpenSearch_Manager()\n",
                "print(f\"사용 가능한 인덱스: {os_manager.index_list}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 챗봇 기능 구현"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def retrieve_search_results(question, bedrock_client, embed_model_id, index_name):\n",
                "    \"\"\"검색 결과 검색 함수\"\"\"\n",
                "    response = bedrock_client.invoke_model(\n",
                "        modelId=embed_model_id,\n",
                "        body=json.dumps({\"inputText\": question})\n",
                "    )\n",
                "    embedding = json.loads(response['body'].read())['embedding']\n",
                "\n",
                "    # Rank Fusion 검색 실행\n",
                "    search_result = os_manager.search_by_rank_fusion(\n",
                "        query_text=question,\n",
                "        vector=embedding,\n",
                "        index_name=index_name,\n",
                "        initial_search_results=160,\n",
                "        hybrid_score_filter=40,\n",
                "        final_reranked_results=DEFAULT_TOP_K,\n",
                "        knn_weight=0.6\n",
                "    )\n",
                "    \n",
                "    return search_result\n",
                "\n",
                "def format_message(role, content):\n",
                "    \"\"\"메시지 포맷팅 함수\"\"\"\n",
                "    return {\"role\": role, \"content\": [{\"text\": content}]}\n",
                "\n",
                "def chat_with_rag(question, history, bedrock_client, model_id, model_kwargs, embed_model_id, index_name):\n",
                "    \"\"\"RAG 기반 챗봇 대화 함수\"\"\"\n",
                "    # 검색 실행\n",
                "    search_result = retrieve_search_results(question, bedrock_client, embed_model_id, index_name)\n",
                "    \n",
                "    # 컨텍스트 준비\n",
                "    context = \"\\n\\n\".join([result['content'] for result in search_result])\n",
                "    \n",
                "    # 시스템 프롬프트 설정\n",
                "    system_prompt = [\n",
                "        {\"text\": \"\"\"당신은 도움이 되는 AI 어시스턴트입니다. \n",
                "        주어진 컨텍스트만을 기반으로 정확하고 관련성 있는 답변을 제공하세요. \n",
                "        컨텍스트에 없는 정보는 사용하지 마세요. \n",
                "        컨텍스트에서 정보를 찾을 수 없는 경우, \n",
                "        답변할 수 있는 충분한 정보가 없다고 말씀해 주세요.\"\"\"}\n",
                "    ]\n",
                "    \n",
                "    # 대화 히스토리 포맷팅\n",
                "    messages = [format_message(msg[\"role\"], msg[\"content\"]) for msg in history]\n",
                "    \n",
                "    # 현재 질문에 컨텍스트 추가\n",
                "    current_question = f\"컨텍스트:\\n{context}\\n\\n질문: {question}\"\n",
                "    messages.append(format_message(\"user\", current_question))\n",
                "    \n",
                "    # Bedrock 모델 호출\n",
                "    response = bedrock_client.converse(\n",
                "        modelId=model_id,\n",
                "        messages=messages,\n",
                "        system=system_prompt,\n",
                "        inferenceConfig={\n",
                "            \"temperature\": model_kwargs[\"temperature\"],\n",
                "            \"topP\": model_kwargs[\"top_p\"],\n",
                "            \"maxTokens\": model_kwargs[\"max_tokens\"]\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    return response['output']['message']['content'][0]['text'], search_result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 대화형 인터페이스"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, clear_output\n",
                "import boto3\n",
                "\n",
                "# Bedrock 클라이언트 설정\n",
                "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
                "model_name = list(model_config['models'].keys())[0]\n",
                "model_id = model_config['models'][model_name]['model_id']\n",
                "embed_model = list(model_config['embedding_models'].keys())[0]\n",
                "embed_model_id = model_config['embedding_models'][embed_model]['model_id']\n",
                "\n",
                "# 모델 설정\n",
                "model_kwargs = {\n",
                "    \"temperature\": DEFAULT_TEMPERATURE,\n",
                "    \"top_p\": DEFAULT_TOP_P,\n",
                "    \"max_tokens\": 4096\n",
                "}\n",
                "\n",
                "# 대화 기록\n",
                "conversation_history = []\n",
                "\n",
                "# UI 구성요소\n",
                "question_input = widgets.Text(description='질문:', layout=widgets.Layout(width='80%'))\n",
                "send_button = widgets.Button(description='전송')\n",
                "output = widgets.Output()\n",
                "clear_button = widgets.Button(description='대화 초기화')\n",
                "\n",
                "def on_send_button_clicked(b):\n",
                "    question = question_input.value\n",
                "    if question.strip():\n",
                "        with output:\n",
                "            print(f\"\\n사용자: {question}\")\n",
                "            response, search_results = chat_with_rag(\n",
                "                question,\n",
                "                conversation_history,\n",
                "                bedrock_client,\n",
                "                model_id,\n",
                "                model_kwargs,\n",
                "                embed_model_id,\n",
                "                f\"aws_{os_manager.index_list[0]}\"\n",
                "            )\n",
                "            print(f\"\\n어시스턴트: {response}\")\n",
                "            print(\"\\n참고한 문서:\")\n",
                "            for i, result in enumerate(search_results[:3], 1):\n",
                "                print(f\"\\n{i}. 관련도: {result['score']:.4f}\")\n",
                "                print(f\"내용: {result['content'][:200]}...\")\n",
                "        \n",
                "        conversation_history.extend([\n",
                "            {\"role\": \"user\", \"content\": question},\n",
                "            {\"role\": \"assistant\", \"content\": response}\n",
                "        ])\n",
                "        question_input.value = ''\n",
                "\n",
                "def on_clear_button_clicked(b):\n",
                "    conversation_history.clear()\n",
                "    with output:\n",
                "        clear_output()\n",
                "        print(\"대화가 초기화되었습니다.\")\n",
                "\n",
                "send_button.on_click(on_send_button_clicked)\n",
                "clear_button.on_click(on_clear_button_clicked)\n",
                "\n",
                "# UI 표시\n",
                "display(widgets.VBox([\n",
                "    widgets.HBox([question_input, send_button, clear_button]),\n",
                "    output\n",
                "]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}